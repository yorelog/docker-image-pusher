use crate::error::{RegistryError, Result};
use crate::registry::tar_utils::TarUtils;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs::{self, File};
use std::io::{Read, Write};
use std::path::{Path, PathBuf};

// ç¼“å­˜ç›®å½•ç»“æ„å¸¸é‡
pub const CACHE_DIR: &str = ".cache";
pub const MANIFESTS_DIR: &str = "manifests";
pub const BLOBS_DIR: &str = "blobs";
pub const SHA256_PREFIX: &str = "sha256";

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct BlobInfo {
    pub digest: String,
    pub size: u64,
    pub path: PathBuf,
    pub is_config: bool,
    pub compressed: bool,
    pub media_type: String, // Add media_type field
}

/// Docker é•œåƒç¼“å­˜ç®¡ç†
///
/// æä¾›æœ¬åœ°ç¼“å­˜åŠŸèƒ½ï¼Œç»“æ„ä¸ Docker Registry API å…¼å®¹ï¼Œæ”¯æŒä» repository æˆ– tar æ–‡ä»¶
/// è·å– manifest å’Œ blobï¼Œå¹¶æ”¯æŒä»ç¼“å­˜ä¸­æ¨é€ã€‚
///
/// ç¼“å­˜ç»“æ„:
/// ```text
/// .cache/
///   manifests/{repository}/{reference}
///   blobs/sha256/{digest}
///   index.json  // ç¼“å­˜ç´¢å¼•
/// ```
pub struct Cache {
    cache_dir: PathBuf,
    index: HashMap<String, CacheEntry>,
}

#[derive(Debug, serde::Serialize, serde::Deserialize)]
struct CacheEntry {
    repository: String,
    reference: String,
    manifest_path: PathBuf,
    config_digest: String,
    blobs: HashMap<String, BlobInfo>,
}

impl Cache {
    /// åˆ›å»ºæ–°çš„ç¼“å­˜ç®¡ç†å™¨
    pub fn new<P: AsRef<Path>>(cache_dir: Option<P>) -> Result<Self> {
        let cache_dir = match cache_dir {
            Some(dir) => PathBuf::from(dir.as_ref()),
            None => PathBuf::from(CACHE_DIR),
        };

        if !cache_dir.exists() {
            fs::create_dir_all(&cache_dir)?;
            // åˆ›å»º manifests å’Œ blobs ç›®å½•
            fs::create_dir_all(cache_dir.join(MANIFESTS_DIR))?;
            fs::create_dir_all(cache_dir.join(BLOBS_DIR).join(SHA256_PREFIX))?;
        }

        let index_path = cache_dir.join("index.json");
        let index = if index_path.exists() {
            let mut file = File::open(&index_path)?;
            let mut contents = String::new();
            file.read_to_string(&mut contents)?;
            serde_json::from_str(&contents)
                .map_err(|e| RegistryError::Parse(format!("Failed to parse cache index: {}", e)))?
        } else {
            HashMap::new()
        };

        Ok(Cache { cache_dir, index })
    }

    /// ä¿å­˜æ¸…å•ï¼ˆmanifestï¼‰åˆ°ç¼“å­˜
    pub fn save_manifest(
        &mut self,
        repository: &str,
        reference: &str,
        manifest: &[u8],
        config_digest: &str,
    ) -> Result<PathBuf> {
        // ç¡®ä¿ç›®å½•ç»“æ„å­˜åœ¨
        let manifest_dir = self.cache_dir.join(MANIFESTS_DIR).join(repository);
        fs::create_dir_all(&manifest_dir)?;

        // ä¿å­˜ manifest æ–‡ä»¶
        let manifest_path = manifest_dir.join(reference);
        let mut file = File::create(&manifest_path)?;
        file.write_all(manifest)?;

        // æ›´æ–°æˆ–åˆ›å»ºç¼“å­˜æ¡ç›®
        let cache_key = format!("{}/{}", repository, reference);
        let entry = self
            .index
            .entry(cache_key.clone())
            .or_insert_with(|| CacheEntry {
                repository: repository.to_string(),
                reference: reference.to_string(),
                manifest_path: manifest_path.clone(),
                config_digest: config_digest.to_string(),
                blobs: HashMap::new(),
            });

        entry.manifest_path = manifest_path.clone();
        entry.config_digest = config_digest.to_string();

        self.save_index()?;

        Ok(manifest_path)
    }

    /// ä¿å­˜ blob åˆ°ç¼“å­˜
    pub fn save_blob(
        &mut self,
        digest: &str,
        data: &[u8],
        _is_config: bool,
        _compressed: bool,
    ) -> Result<PathBuf> {
        // æ ‡å‡†åŒ–æ‘˜è¦æ ¼å¼ (ç¡®ä¿æœ‰ sha256: å‰ç¼€)
        let normalized_digest = self.normalize_digest(digest);
        let digest_value = normalized_digest
            .split(':')
            .nth(1)
            .unwrap_or(&normalized_digest);

        // åˆ›å»º blob ç›®å½•
        let blob_dir = self.cache_dir.join(BLOBS_DIR).join(SHA256_PREFIX);
        fs::create_dir_all(&blob_dir)?;

        // ä¿å­˜ blob æ–‡ä»¶
        let blob_path = blob_dir.join(digest_value);

        if blob_path.exists() {
            // å¦‚æœ blob å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦åŒ¹é… (ç®€å•éªŒè¯)
            let metadata = fs::metadata(&blob_path)?;
            if metadata.len() == data.len() as u64 {
                return Ok(blob_path);
            }
        }

        let mut file = File::create(&blob_path)?;
        file.write_all(data)?;

        // è®°å½• blob ä¿¡æ¯ï¼Œä½†ä¸ä¸ç‰¹å®šé•œåƒå…³è”ï¼ˆé€šè¿‡ manifest å…³è”ï¼‰

        Ok(blob_path)
    }

    /// å°† blob å…³è”åˆ°æŒ‡å®šçš„é•œåƒ
    pub fn associate_blob_with_image(
        &mut self,
        repository: &str,
        reference: &str,
        digest: &str,
        size: u64,
        is_config: bool,
        compressed: bool,
    ) -> Result<()> {
        let normalized_digest = self.normalize_digest(digest);
        let cache_key = format!("{}/{}", repository, reference);

        if let Some(entry) = self.index.get_mut(&cache_key) {
            // è·å– blob æ–‡ä»¶è·¯å¾„
            let digest_value = normalized_digest
                .split(':')
                .nth(1)
                .unwrap_or(&normalized_digest);
            let blob_path = self
                .cache_dir
                .join(BLOBS_DIR)
                .join(SHA256_PREFIX)
                .join(digest_value);

            // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if !blob_path.exists() {
                return Err(RegistryError::Cache {
                    message: format!("Blob {} not found in cache", normalized_digest),
                    path: Some(blob_path),
                });
            }

            entry.blobs.insert(
                normalized_digest.clone(),
                BlobInfo {
                    digest: normalized_digest,
                    size,
                    path: blob_path,
                    is_config,
                    compressed,
                    media_type: String::new(), // Default to empty media_type
                },
            );

            self.save_index()?;
            Ok(())
        } else {
            Err(RegistryError::Cache {
                message: format!("Image {}/{} not found in cache", repository, reference),
                path: None,
            })
        }
    }

    /// Add a blob to the cache
    pub fn add_blob(
        &mut self,
        digest: &str,
        data: &[u8],
        _is_config: bool,
        _compressed: bool,
    ) -> Result<PathBuf> {
        if !digest.starts_with("sha256:") {
            return Err(RegistryError::Validation(
                "Blob digest must start with sha256:".into(),
            ));
        }

        // Verify the blob digest
        let actual_digest = format!(
            "sha256:{}",
            crate::image::digest::DigestUtils::compute_sha256(data)
        );
        if actual_digest != digest {
            return Err(RegistryError::Validation(format!(
                "Blob digest mismatch. Expected: {}, Got: {}",
                digest, actual_digest
            )));
        }

        let blob_path = self.get_blob_path(digest);
        if !blob_path.exists() {
            // Ensure parent directories exist
            if let Some(parent) = blob_path.parent() {
                fs::create_dir_all(parent)?;
            }

            // Write blob data
            let mut file = File::create(&blob_path)?;
            file.write_all(data)?;
        }

        Ok(blob_path)
    }

    /// Get blob path from digest
    pub fn get_blob_path(&self, digest: &str) -> PathBuf {
        let digest = digest.trim_start_matches("sha256:");
        self.cache_dir
            .join(BLOBS_DIR)
            .join(SHA256_PREFIX)
            .join(digest)
    }

    /// Check if a blob exists in cache
    pub fn has_blob(&self, digest: &str) -> bool {
        self.get_blob_path(digest).exists()
    }

    /// Check if a blob exists in cache with enhanced integrity verification
    pub fn has_blob_with_verification(&self, digest: &str, verify_integrity: bool) -> bool {
        let blob_path = self.get_blob_path(digest);
        
        if !blob_path.exists() {
            return false;
        }

        // è·å–æ–‡ä»¶å…ƒæ•°æ®
        let metadata = match fs::metadata(&blob_path) {
            Ok(meta) => meta,
            Err(_) => return false,
        };

        let file_size = metadata.len();

        // å¯¹äºå°æ–‡ä»¶æˆ–å¼ºåˆ¶éªŒè¯ï¼Œè¿›è¡Œå®Œæ•´SHA256éªŒè¯
        if verify_integrity || file_size <= 10 * 1024 * 1024 {
            match self.verify_blob_integrity(&blob_path, digest) {
                Ok(valid) => valid,
                Err(_) => false,
            }
        } else {
            // å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨è½»é‡çº§å®Œæ•´æ€§æ£€æŸ¥
            self.verify_large_file_integrity(&blob_path, digest, file_size)
        }
    }

    /// Verify blob integrity using full SHA256 calculation
    fn verify_blob_integrity(&self, blob_path: &Path, expected_digest: &str) -> Result<bool> {
        let data = fs::read(blob_path)?;
        let actual_digest = format!(
            "sha256:{}",
            crate::image::digest::DigestUtils::compute_sha256(&data)
        );
        Ok(actual_digest == expected_digest)
    }

    /// Verify large file integrity using lightweight checks
    fn verify_large_file_integrity(&self, blob_path: &Path, _digest: &str, file_size: u64) -> bool {
        // åŸºç¡€æ£€æŸ¥ï¼šæ–‡ä»¶å¤§å°æ˜¯å¦åˆç†
        if file_size == 0 {
            return false;
        }

        // æ£€æŸ¥æ–‡ä»¶å¤´éƒ¨æ˜¯å¦ä¸ºæœ‰æ•ˆçš„gzipæ ¼å¼ï¼ˆå¤§å¤šæ•°Dockerå±‚éƒ½æ˜¯gzipå‹ç¼©çš„ï¼‰
        if let Ok(mut file) = File::open(blob_path) {
            let mut header = [0u8; 10];
            if let Ok(n) = file.read(&mut header) {
                if n >= 2 {
                    // æ£€æŸ¥gzipé­”æ•° (0x1f, 0x8b)
                    if header[0] == 0x1f && header[1] == 0x8b {
                        // å¯¹äºè¶…å¤§æ–‡ä»¶ï¼Œè¿›è¡ŒæŠ½æ ·éªŒè¯
                        if file_size > 100 * 1024 * 1024 {
                            return self.verify_large_file_sampling(blob_path, file_size);
                        }
                        return true;
                    }
                }
            }
        }

        // å¦‚æœä¸æ˜¯gzipæ ¼å¼ï¼Œå¯èƒ½æ˜¯å…¶ä»–æœ‰æ•ˆæ ¼å¼ï¼Œè¿›è¡ŒåŸºç¡€éªŒè¯
        self.verify_file_basic_integrity(blob_path, file_size)
    }

    /// Perform sampling verification for very large files
    fn verify_large_file_sampling(&self, blob_path: &Path, file_size: u64) -> bool {
        use std::io::Seek;
        
        if let Ok(mut file) = File::open(blob_path) {
            let sample_size = 4096;
            let mut buffer = vec![0u8; sample_size];

            // æ£€æŸ¥æ–‡ä»¶å¼€å¤´
            if file.read(&mut buffer).is_err() {
                return false;
            }

            // æ£€æŸ¥æ˜¯å¦å…¨ä¸ºé›¶å­—èŠ‚ï¼ˆæŸåçš„æ–‡ä»¶æ¨¡å¼ï¼‰
            if buffer.iter().all(|&b| b == 0) {
                return false;
            }

            // æ£€æŸ¥æ–‡ä»¶ä¸­é—´éƒ¨åˆ†
            let middle_pos = file_size / 2;
            if let Ok(_) = file.seek(std::io::SeekFrom::Start(middle_pos)) {
                if file.read(&mut buffer).is_err() {
                    return false;
                }
                // åŒæ ·æ£€æŸ¥é›¶å­—èŠ‚æ¨¡å¼
                if buffer.iter().all(|&b| b == 0) {
                    return false;
                }
            }

            // æ£€æŸ¥æ–‡ä»¶æœ«å°¾
            let end_pos = file_size.saturating_sub(sample_size as u64);
            if let Ok(_) = file.seek(std::io::SeekFrom::Start(end_pos)) {
                if file.read(&mut buffer).is_err() {
                    return false;
                }
                // æ£€æŸ¥é›¶å­—èŠ‚æ¨¡å¼
                if buffer.iter().all(|&b| b == 0) {
                    return false;
                }
            }

            true
        } else {
            false
        }
    }

    /// Basic file integrity verification
    fn verify_file_basic_integrity(&self, blob_path: &Path, file_size: u64) -> bool {
        // åŸºç¡€æ£€æŸ¥ï¼šæ–‡ä»¶å¤§å°åˆç†æ€§
        if file_size == 0 || file_size > 10 * 1024 * 1024 * 1024 {
            // æ‹’ç»ç©ºæ–‡ä»¶æˆ–è¶…è¿‡10GBçš„æ–‡ä»¶
            return false;
        }

        // ç®€å•çš„è¯»å–æµ‹è¯•
        if let Ok(mut file) = File::open(blob_path) {
            let mut buffer = [0u8; 1024];
            // å°è¯•è¯»å–æ–‡ä»¶å¼€å¤´ä»¥ç¡®ä¿æ–‡ä»¶å¯è¯»
            file.read(&mut buffer).is_ok()
        } else {
            false
        }
    }

    /// ä»ç¼“å­˜ä¸­è·å– manifest
    pub fn get_manifest(&self, repository: &str, reference: &str) -> Result<Vec<u8>> {
        let cache_key = format!("{}/{}", repository, reference);
        if let Some(entry) = self.index.get(&cache_key) {
            if entry.manifest_path.exists() {
                return Ok(fs::read(&entry.manifest_path)?);
            }
        }
        Err(RegistryError::NotFound(format!(
            "Manifest not found for {}/{}",
            repository, reference
        )))
    }

    /// ä»ç¼“å­˜ä¸­è·å– blob
    pub fn get_blob(&self, digest: &str) -> Result<Vec<u8>> {
        let blob_path = self.get_blob_path(digest);
        if blob_path.exists() {
            Ok(fs::read(blob_path)?)
        } else {
            Err(RegistryError::NotFound(format!(
                "Blob not found: {}",
                digest
            )))
        }
    }

    /// åˆ é™¤æ¸…å•åŠå…¶æœªè¢«å…¶ä»–é•œåƒå¼•ç”¨çš„ blob
    pub fn remove_manifest(&mut self, repository: &str, reference: &str) -> Result<()> {
        let cache_key = format!("{}/{}", repository, reference);
        if let Some(entry) = self.index.remove(&cache_key) {
            // åˆ é™¤ manifest æ–‡ä»¶
            if entry.manifest_path.exists() {
                fs::remove_file(&entry.manifest_path)?;
            }

            // æ¸…ç†æœªè¢«å¼•ç”¨çš„ blob
            self.cleanup_unreferenced_blobs()?;
        }
        self.save_index()
    }

    /// æ¸…ç†æœªè¢«å¼•ç”¨çš„ blob
    fn cleanup_unreferenced_blobs(&self) -> Result<()> {
        let mut referenced_blobs: HashMap<String, bool> = HashMap::new();

        // æ”¶é›†æ‰€æœ‰å¼•ç”¨çš„ blob
        for entry in self.index.values() {
            for blob_info in entry.blobs.values() {
                referenced_blobs.insert(blob_info.digest.clone(), true);
            }
        }

        // æ£€æŸ¥å¹¶åˆ é™¤æœªè¢«å¼•ç”¨çš„ blob
        let blobs_dir = self.cache_dir.join(BLOBS_DIR).join(SHA256_PREFIX);
        if blobs_dir.exists() {
            for entry in fs::read_dir(blobs_dir)? {
                let entry = entry?;
                let digest = format!("sha256:{}", entry.file_name().to_string_lossy());
                if !referenced_blobs.contains_key(&digest) {
                    fs::remove_file(entry.path())?;
                }
            }
        }

        Ok(())
    }

    /// åˆ—å‡ºç¼“å­˜ä¸­çš„æ‰€æœ‰æ¸…å•
    pub fn list_manifests(&self) -> Vec<(String, String)> {
        self.index
            .iter()
            .map(|(_, entry)| (entry.repository.clone(), entry.reference.clone()))
            .collect()
    }

    /// è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> Result<CacheStats> {
        let mut stats = CacheStats {
            manifest_count: self.index.len(),
            blob_count: 0,
            total_size: 0,
        };

        // è®¡ç®— blob ç»Ÿè®¡ä¿¡æ¯
        let blobs_dir = self.cache_dir.join(BLOBS_DIR).join(SHA256_PREFIX);
        if blobs_dir.exists() {
            for entry in fs::read_dir(blobs_dir)? {
                let entry = entry?;
                if entry.file_type()?.is_file() {
                    stats.blob_count += 1;
                    stats.total_size += entry.metadata()?.len();
                }
            }
        }

        Ok(stats)
    }

    /// ä¿å­˜ç´¢å¼•æ–‡ä»¶
    fn save_index(&self) -> Result<()> {
        let index_path = self.cache_dir.join("index.json");
        let json_data = serde_json::to_string_pretty(&self.index)
            .map_err(|e| RegistryError::Parse(format!("Failed to serialize cache index: {}", e)))?;

        let mut file = File::create(&index_path)?;
        file.write_all(json_data.as_bytes())?;
        Ok(())
    }

    /// æ ‡å‡†åŒ–æ‘˜è¦æ ¼å¼
    fn normalize_digest(&self, digest: &str) -> String {
        if digest.starts_with("sha256:") {
            digest.to_string()
        } else {
            format!("sha256:{}", digest)
        }
    }

    /// ä»ç¼“å­˜ä¸­è·å–blobå¤§å°
    pub fn get_blob_size(&self, digest: &str) -> Option<u64> {
        let blob_path = self.get_blob_path(digest);
        if blob_path.exists() {
            if let Ok(metadata) = fs::metadata(&blob_path) {
                return Some(metadata.len());
            }
        }
        None
    }

    /// è·å–é•œåƒçš„æ‰€æœ‰blobä¿¡æ¯
    pub fn get_image_blobs(&self, repository: &str, reference: &str) -> Result<Vec<BlobInfo>> {
        let cache_key = format!("{}/{}", repository, reference);
        if let Some(entry) = self.index.get(&cache_key) {
            Ok(entry.blobs.values().cloned().collect())
        } else {
            Err(RegistryError::NotFound(format!(
                "Image {}/{} not found in cache",
                repository, reference
            )))
        }
    }

    /// æ£€æŸ¥é•œåƒæ˜¯å¦å®Œæ•´ï¼ˆmanifestå’Œæ‰€æœ‰blobéƒ½å­˜åœ¨ï¼‰
    pub fn is_image_complete(&self, repository: &str, reference: &str) -> Result<bool> {
        let cache_key = format!("{}/{}", repository, reference);
        if let Some(entry) = self.index.get(&cache_key) {
            // æ£€æŸ¥manifestæ–‡ä»¶å­˜åœ¨
            if !entry.manifest_path.exists() {
                return Ok(false);
            }

            // æ£€æŸ¥æ‰€æœ‰blobå­˜åœ¨
            for blob_info in entry.blobs.values() {
                if !blob_info.path.exists() {
                    return Ok(false);
                }
            }

            Ok(true)
        } else {
            Ok(false)
        }
    }
    /// ä»taræ–‡ä»¶ä¸­æå–blobä¿¡æ¯å¹¶ç¼“å­˜
    pub fn cache_from_tar(
        &mut self,
        tar_path: &Path,
        repository: &str,
        reference: &str,
    ) -> Result<()> {
        // ç›´æ¥ä½¿ç”¨TarUtilsè§£ætaræ–‡ä»¶
        let image_info = TarUtils::parse_image_info(tar_path)?;

        // åˆ›å»ºç®€åŒ–çš„manifestç»“æ„å¹¶ä¿å­˜
        let manifest_json = self.create_manifest_from_image_info(&image_info)?;
        self.save_manifest(
            repository,
            reference,
            manifest_json.as_bytes(),
            &image_info.config_digest,
        )?;

        // ç¼“å­˜config blob
        let config_data = TarUtils::extract_config_data(tar_path, &image_info.config_digest)?;
        self.save_blob(&image_info.config_digest, &config_data, true, false)?;
        self.associate_blob_with_image(
            repository,
            reference,
            &image_info.config_digest,
            config_data.len() as u64,
            true,
            false,
        )?;

        // Cache all layer blobs
        for layer in &image_info.layers {
            let layer_data = TarUtils::extract_layer_data(tar_path, &layer.tar_path)?;
            // Layer data is already in correct gzip format, save directly
            self.save_blob(&layer.digest, &layer_data, false, true)?;
            self.associate_blob_with_image(
                repository,
                reference,
                &layer.digest,
                layer_data.len() as u64,
                false,
                true,
            )?;
        }

        Ok(())
    }

    /// ä»ImageInfoåˆ›å»ºDocker v2 manifest
    fn create_manifest_from_image_info(
        &self,
        image_info: &crate::image::parser::ImageInfo,
    ) -> Result<String> {
        let config = serde_json::json!({
            "mediaType": "application/vnd.docker.container.image.v1+json",
            "size": image_info.config_size,
            "digest": image_info.config_digest
        });

        let layers: Vec<serde_json::Value> = image_info
            .layers
            .iter()
            .map(|layer| {
                serde_json::json!({
                    "mediaType": "application/vnd.docker.image.rootfs.diff.tar.gzip",
                    "size": layer.size,
                    "digest": layer.digest
                })
            })
            .collect();

        let manifest = serde_json::json!({
            "schemaVersion": 2,
            "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
            "config": config,
            "layers": layers
        });

        Ok(serde_json::to_string_pretty(&manifest)?)
    }

    /// Add a blob to the cache with enhanced integrity verification
    pub async fn add_blob_with_verification(
        &mut self,
        digest: &str,
        data: &[u8],
        is_config: bool,
        _compressed: bool, // æ ‡è®°ä¸ºæœªä½¿ç”¨ä½†ä¿ç•™æ¥å£å…¼å®¹æ€§
        force_verify: bool,
    ) -> Result<PathBuf> {
        if !digest.starts_with("sha256:") {
            return Err(RegistryError::Validation(
                "Blob digest must start with sha256:".into(),
            ));
        }

        let blob_path = self.get_blob_path(digest);
        
        // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ (å¢å¼ºå®Œæ•´æ€§æ£€æŸ¥)
        if blob_path.exists() {
            if let Ok(existing_metadata) = fs::metadata(&blob_path) {
                if existing_metadata.len() == data.len() as u64 && existing_metadata.len() > 0 {
                    // æ ¹æ®æ–‡ä»¶å¤§å°å’Œå¼ºåˆ¶éªŒè¯æ ‡å¿—å†³å®šéªŒè¯ç­–ç•¥
                    let should_verify = force_verify || is_config || data.len() <= 10 * 1024 * 1024;
                    
                    if should_verify {
                        // å°æ–‡ä»¶æˆ–å¼ºåˆ¶éªŒè¯ - å®Œæ•´SHA256éªŒè¯
                        if let Ok(existing_data) = fs::read(&blob_path) {
                            let expected_digest_value = digest.trim_start_matches("sha256:");
                            let existing_digest = crate::image::digest::DigestUtils::compute_sha256(&existing_data);
                            if existing_digest == expected_digest_value {
                                return Ok(blob_path);
                            }
                        }
                    } else {
                        // å¤§æ–‡ä»¶ - ä½¿ç”¨å¢å¼ºçš„å®Œæ•´æ€§æ£€æŸ¥
                        if self.verify_large_file_integrity(&blob_path, digest, existing_metadata.len()) {
                            return Ok(blob_path);
                        }
                    }
                }
            }
            // å¦‚æœéªŒè¯å¤±è´¥ï¼Œåˆ é™¤æŸåçš„æ–‡ä»¶
            let _ = fs::remove_file(&blob_path);
        }

        // å¯¹äºæ–°blobï¼Œè¿›è¡Œé€‚å½“çš„å®Œæ•´æ€§éªŒè¯
        if force_verify || is_config || data.len() <= 10 * 1024 * 1024 {
            // å°æ–‡ä»¶æˆ–å¼ºåˆ¶éªŒè¯ - å®Œæ•´SHA256éªŒè¯
            let actual_digest = format!(
                "sha256:{}",
                crate::image::digest::DigestUtils::compute_sha256(data)
            );
            if actual_digest != digest {
                return Err(RegistryError::Validation(format!(
                    "Blob digest mismatch. Expected: {}, Got: {}",
                    digest, actual_digest
                )));
            }
        } else {
            // å¤§æ–‡ä»¶ - åŸºç¡€éªŒè¯ (æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ä½†ä¸è®¡ç®—SHA256)
            if data.is_empty() {
                return Err(RegistryError::Validation(
                    "Cannot cache empty blob data".to_string(),
                ));
            }
        }

        // Ensure parent directories exist
        if let Some(parent) = blob_path.parent() {
            fs::create_dir_all(parent)?;
        }

        // ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¡®ä¿åŸå­å†™å…¥
        let temp_path = blob_path.with_extension("tmp");
        let mut file = File::create(&temp_path)?;
        file.write_all(data)?;
        file.sync_all()?;

        // åŸå­æ€§åœ°é‡å‘½åä¸´æ—¶æ–‡ä»¶
        fs::rename(&temp_path, &blob_path)?;

        Ok(blob_path)
    }

    /// æµå¼ä¿å­˜blobåˆ°ç¼“å­˜ï¼Œæ”¯æŒè¾¹ä¸‹è½½è¾¹éªŒè¯
    pub async fn add_blob_stream_with_verification<R>(
        &mut self,
        digest: &str,
        mut reader: R,
        expected_size: Option<u64>,
        _is_config: bool, // ä¿ç•™æ¥å£å…¼å®¹æ€§ä½†æ ‡è®°ä¸ºæœªä½¿ç”¨
        verify_integrity: bool,
    ) -> Result<PathBuf>
    where
        R: tokio::io::AsyncRead + Unpin,
    {
        use tokio::io::AsyncReadExt;
        use sha2::{Sha256, Digest};
        
        if !digest.starts_with("sha256:") {
            return Err(RegistryError::Validation(
                "Blob digest must start with sha256:".into(),
            ));
        }

        let blob_path = self.get_blob_path(digest);
        
        // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if blob_path.exists() {
            if let Ok(metadata) = fs::metadata(&blob_path) {
                if let Some(expected) = expected_size {
                    if metadata.len() == expected && metadata.len() > 0 {
                        // ä½¿ç”¨å¢å¼ºçš„å®Œæ•´æ€§æ£€æŸ¥
                        if self.has_blob_with_verification(digest, verify_integrity) {
                            return Ok(blob_path);
                        }
                    }
                }
            }
            // å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œåˆ é™¤ç°æœ‰æ–‡ä»¶
            let _ = fs::remove_file(&blob_path);
        }

        // Ensure parent directories exist
        if let Some(parent) = blob_path.parent() {
            fs::create_dir_all(parent)?;
        }

        // ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡Œæµå¼å†™å…¥
        let temp_path = blob_path.with_extension("tmp");
        let mut temp_file = tokio::fs::File::create(&temp_path).await
            .map_err(|e| RegistryError::Io(format!("Failed to create temp file: {}", e)))?;

        let mut total_written = 0u64;
        let mut hasher: Option<Sha256> = if verify_integrity {
            Some(Sha256::new())
        } else {
            None
        };

        // æµå¼è¯»å–å’Œå†™å…¥
        let mut chunk = [0u8; 8192];
        loop {
            let n = reader.read(&mut chunk).await
                .map_err(|e| RegistryError::Io(format!("Failed to read stream: {}", e)))?;
            
            if n == 0 {
                break;
            }

            let chunk_data = &chunk[..n];
            
            // å†™å…¥ä¸´æ—¶æ–‡ä»¶
            tokio::io::AsyncWriteExt::write_all(&mut temp_file, chunk_data).await
                .map_err(|e| RegistryError::Io(format!("Failed to write to temp file: {}", e)))?;
            
            // æ›´æ–°å“ˆå¸Œè®¡ç®— (å¦‚æœéœ€è¦éªŒè¯)
            if let Some(ref mut h) = hasher {
                h.update(chunk_data);
            }
            
            total_written += n as u64;
            
            // æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦è¶…å‡ºé¢„æœŸ
            if let Some(expected) = expected_size {
                if total_written > expected {
                    let _ = tokio::fs::remove_file(&temp_path).await;
                    return Err(RegistryError::Validation(format!(
                        "Blob size exceeded expected: {} > {}",
                        total_written, expected
                    )));
                }
            }
        }

        // ç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
        temp_file.sync_all().await
            .map_err(|e| RegistryError::Io(format!("Failed to sync temp file: {}", e)))?;

        // éªŒè¯æ–‡ä»¶å¤§å°
        if let Some(expected) = expected_size {
            if total_written != expected {
                let _ = tokio::fs::remove_file(&temp_path).await;
                return Err(RegistryError::Validation(format!(
                    "Blob size mismatch: expected {}, got {}",
                    expected, total_written
                )));
            }
        }

        // éªŒè¯SHA256 (å¦‚æœéœ€è¦)
        if let Some(h) = hasher {
            let computed_digest = format!("sha256:{}", hex::encode(h.finalize()));
            if computed_digest != digest {
                let _ = tokio::fs::remove_file(&temp_path).await;
                return Err(RegistryError::Validation(format!(
                    "Stream digest mismatch. Expected: {}, Got: {}",
                    digest, computed_digest
                )));
            }
        }

        // åŸå­æ€§åœ°é‡å‘½åä¸´æ—¶æ–‡ä»¶
        tokio::fs::rename(&temp_path, &blob_path).await
            .map_err(|e| RegistryError::Io(format!("Failed to rename temp file: {}", e)))?;

        Ok(blob_path)
    }

    /// Comprehensive blob verification with detailed logging for debugging
    pub fn verify_blob_exists_with_details(&self, digest: &str, verbose_output: Option<&crate::logging::Logger>) -> Result<(bool, String)> {
        let normalized_digest = if digest.starts_with("sha256:") {
            digest.to_string()
        } else {
            format!("sha256:{}", digest)
        };
        
        let blob_path = self.get_blob_path(&normalized_digest);
        let mut debug_info = Vec::new();
        
        debug_info.push(format!("ğŸ” Verifying blob: {}", &normalized_digest[..23]));
        debug_info.push(format!("ğŸ“ Expected path: {}", blob_path.display()));
        
        // Check if file exists
        if !blob_path.exists() {
            debug_info.push("âŒ File does not exist in cache".to_string());
            let report = debug_info.join("\n");
            if let Some(logger) = verbose_output {
                logger.warning(&report);
            }
            return Ok((false, report));
        }
        
        // Get file metadata
        let metadata = match fs::metadata(&blob_path) {
            Ok(meta) => {
                debug_info.push(format!("ğŸ“Š File size: {} bytes", meta.len()));
                meta
            },
            Err(e) => {
                debug_info.push(format!("âŒ Failed to read file metadata: {}", e));
                let report = debug_info.join("\n");
                if let Some(logger) = verbose_output {
                    logger.warning(&report);
                }
                return Ok((false, report));
            }
        };
        
        let file_size = metadata.len();
        
        // Check if file is empty
        if file_size == 0 {
            debug_info.push("âŒ File is empty (0 bytes)".to_string());
            let report = debug_info.join("\n");
            if let Some(logger) = verbose_output {
                logger.warning(&report);
            }
            return Ok((false, report));
        }
        
        // Read file and verify SHA256
        let file_data = match fs::read(&blob_path) {
            Ok(data) => {
                debug_info.push(format!("âœ… Successfully read {} bytes from file", data.len()));
                data
            },
            Err(e) => {
                debug_info.push(format!("âŒ Failed to read file content: {}", e));
                let report = debug_info.join("\n");
                if let Some(logger) = verbose_output {
                    logger.warning(&report);
                }
                return Ok((false, report));
            }
        };
        
        // Calculate SHA256 digest
        let calculated_digest = format!(
            "sha256:{}",
            crate::image::digest::DigestUtils::compute_sha256(&file_data)
        );
        
        debug_info.push(format!("ğŸ” Expected digest: {}", normalized_digest));
        debug_info.push(format!("ğŸ” Calculated digest: {}", calculated_digest));
        
        let is_valid = calculated_digest == normalized_digest;
        
        if is_valid {
            debug_info.push("âœ… Digest verification PASSED".to_string());
        } else {
            debug_info.push("âŒ Digest verification FAILED - content corrupted or wrong file".to_string());
        }
        
        let report = debug_info.join("\n");
        if let Some(logger) = verbose_output {
            if is_valid {
                logger.detail(&report);
            } else {
                logger.warning(&report);
            }
        }
        
        Ok((is_valid, report))
    }

    /// Get detailed information about cached image including blob details
    pub fn get_image_cache_details(&self, repository: &str, reference: &str) -> Result<String> {
        let cache_key = format!("{}/{}", repository, reference);
        if let Some(entry) = self.index.get(&cache_key) {
            let mut details = Vec::new();
            details.push(format!("ğŸ“‹ Image: {}/{}", repository, reference));
            details.push(format!("ğŸ“„ Manifest path: {}", entry.manifest_path.display()));
            details.push(format!("ğŸ”§ Config digest: {}", &entry.config_digest[..23]));
            details.push(format!("ğŸ“¦ Total blobs: {}", entry.blobs.len()));
            
            // Check manifest file
            if entry.manifest_path.exists() {
                if let Ok(manifest_data) = fs::read(&entry.manifest_path) {
                    details.push(format!("âœ… Manifest file exists ({} bytes)", manifest_data.len()));
                } else {
                    details.push("âŒ Manifest file exists but unreadable".to_string());
                }
            } else {
                details.push("âŒ Manifest file missing".to_string());
            }
            
            // Check each blob
            for (i, (digest, blob_info)) in entry.blobs.iter().enumerate() {
                details.push(format!("\nğŸ“¦ Blob {}/{}: {}", i + 1, entry.blobs.len(), &digest[..23]));
                details.push(format!("   ğŸ“ Path: {}", blob_info.path.display()));
                details.push(format!("   ğŸ“Š Expected size: {} bytes", blob_info.size));
                details.push(format!("   ğŸ·ï¸  Type: {} (compressed: {})", 
                    if blob_info.is_config { "config" } else { "layer" }, 
                    blob_info.compressed
                ));
                
                if blob_info.path.exists() {
                    if let Ok(metadata) = fs::metadata(&blob_info.path) {
                        let actual_size = metadata.len();
                        if actual_size == blob_info.size {
                            details.push(format!("   âœ… File exists, size matches ({} bytes)", actual_size));
                        } else {
                            details.push(format!("   âš ï¸  File exists but size mismatch: expected {} bytes, actual {} bytes", 
                                blob_info.size, actual_size));
                        }
                    } else {
                        details.push("   âŒ File exists but metadata unreadable".to_string());
                    }
                } else {
                    details.push("   âŒ File missing from cache".to_string());
                }
            }
            
            Ok(details.join("\n"))
        } else {
            Err(RegistryError::NotFound(format!(
                "Image {}/{} not found in cache index",
                repository, reference
            )))
        }
    }

    // ...existing code...
}

#[derive(Debug)]
pub struct CacheStats {
    pub manifest_count: usize,
    pub blob_count: usize,
    pub total_size: u64,
}
